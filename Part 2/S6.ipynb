{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yRE9v0hEH2Cz","executionInfo":{"status":"ok","timestamp":1709355285298,"user_tz":-240,"elapsed":3698,"user":{"displayName":"Aakash Vardhan","userId":"03146811293329168388"}},"outputId":"51df9907-67c8-42d9-8139-8f43c066c1e8"},"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 's6-backprop'...\n","remote: Enumerating objects: 432, done.\u001b[K\n","remote: Counting objects: 100% (210/210), done.\u001b[K\n","remote: Compressing objects: 100% (137/137), done.\u001b[K\n","remote: Total 432 (delta 123), reused 156 (delta 69), pack-reused 222\u001b[K\n","Receiving objects: 100% (432/432), 23.52 MiB | 13.77 MiB/s, done.\n","Resolving deltas: 100% (259/259), done.\n"]}],"source":["!git clone 'https://github.com/aakashvardhan/s6-backprop.git'"]},{"cell_type":"code","source":["import sys\n","sys.path.append('/content/s6-backprop/Part 2')"],"metadata":{"id":"vu-V9vQsIFza","executionInfo":{"status":"ok","timestamp":1709355285298,"user_tz":-240,"elapsed":2,"user":{"displayName":"Aakash Vardhan","userId":"03146811293329168388"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["# Importing torchvision for handling datasets and applying transformations\n","from torchvision import datasets, transforms\n","\n","# Importing PyTorch Library\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","# Importing python files, including CNN model, train & test the model\n","from model import Net, model_summary, test_model_sanity\n","from utils import train, test, plt_fig\n","\n","# Import the Matplotlib library for plotting\n","import matplotlib.pyplot as plt\n","\n","!pip install torchsummary"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LrPib8-OIWhu","executionInfo":{"status":"ok","timestamp":1709355304659,"user_tz":-240,"elapsed":19362,"user":{"displayName":"Aakash Vardhan","userId":"03146811293329168388"}},"outputId":"ba1e74ff-9bb0-4191-d8c9-0cab0163a167"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: torchsummary in /usr/local/lib/python3.10/dist-packages (1.5.1)\n"]}]},{"cell_type":"code","source":["# Check if CUDA is available on the system and set `use_cuda` accordingly\n","use_cuda = torch.cuda.is_available()\n","\n","# Set the device to \"cuda\" if CUDA is available, otherwise fall back to using the CPU\n","device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n","\n","# Initialize the model and move it to the specified device (either GPU or CPU)\n","model = Net().to(device)"],"metadata":{"id":"5mE7I_1TIefo","executionInfo":{"status":"ok","timestamp":1709355304660,"user_tz":-240,"elapsed":5,"user":{"displayName":"Aakash Vardhan","userId":"03146811293329168388"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["model_summary(model)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DNcDKMxaImZj","executionInfo":{"status":"ok","timestamp":1709355306113,"user_tz":-240,"elapsed":1457,"user":{"displayName":"Aakash Vardhan","userId":"03146811293329168388"}},"outputId":"29b42036-7bd9-4b03-b2a6-5047d2c2a9c6"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["----------------------------------------------------------------\n","        Layer (type)               Output Shape         Param #\n","================================================================\n","            Conv2d-1           [-1, 16, 26, 26]             160\n","       BatchNorm2d-2           [-1, 16, 26, 26]              32\n","           Dropout-3           [-1, 16, 26, 26]               0\n","         ConvBlock-4           [-1, 16, 26, 26]               0\n","            Conv2d-5           [-1, 16, 24, 24]           2,320\n","       BatchNorm2d-6           [-1, 16, 24, 24]              32\n","           Dropout-7           [-1, 16, 24, 24]               0\n","         ConvBlock-8           [-1, 16, 24, 24]               0\n","            Conv2d-9           [-1, 16, 22, 22]           2,320\n","      BatchNorm2d-10           [-1, 16, 22, 22]              32\n","          Dropout-11           [-1, 16, 22, 22]               0\n","        ConvBlock-12           [-1, 16, 22, 22]               0\n","           Conv2d-13           [-1, 32, 20, 20]           4,640\n","      BatchNorm2d-14           [-1, 32, 20, 20]              64\n","          Dropout-15           [-1, 32, 20, 20]               0\n","        ConvBlock-16           [-1, 32, 20, 20]               0\n","           Conv2d-17           [-1, 16, 20, 20]             528\n","        MaxPool2d-18           [-1, 16, 10, 10]               0\n","  MaxPoolingBlock-19           [-1, 16, 10, 10]               0\n","           Conv2d-20             [-1, 16, 8, 8]           2,320\n","      BatchNorm2d-21             [-1, 16, 8, 8]              32\n","          Dropout-22             [-1, 16, 8, 8]               0\n","        ConvBlock-23             [-1, 16, 8, 8]               0\n","           Conv2d-24             [-1, 16, 6, 6]           2,320\n","      BatchNorm2d-25             [-1, 16, 6, 6]              32\n","          Dropout-26             [-1, 16, 6, 6]               0\n","        ConvBlock-27             [-1, 16, 6, 6]               0\n","           Conv2d-28             [-1, 32, 4, 4]           4,640\n","      BatchNorm2d-29             [-1, 32, 4, 4]              64\n","          Dropout-30             [-1, 32, 4, 4]               0\n","        ConvBlock-31             [-1, 32, 4, 4]               0\n","        AvgPool2d-32             [-1, 32, 1, 1]               0\n","           Linear-33                   [-1, 10]             330\n","================================================================\n","Total params: 19,866\n","Trainable params: 19,866\n","Non-trainable params: 0\n","----------------------------------------------------------------\n","Input size (MB): 0.00\n","Forward/backward pass size (MB): 1.38\n","Params size (MB): 0.08\n","Estimated Total Size (MB): 1.46\n","----------------------------------------------------------------\n"]}]},{"cell_type":"code","source":["test_model_sanity()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":734},"id":"SjJx8w5IddRV","executionInfo":{"status":"error","timestamp":1709355308210,"user_tz":-240,"elapsed":2101,"user":{"displayName":"Aakash Vardhan","userId":"03146811293329168388"}},"outputId":"7affbbc9-9d7c-4334-c5e4-12b4667a899f"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n","Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to data/MNIST/raw/train-images-idx3-ubyte.gz\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 9912422/9912422 [00:00<00:00, 400814738.97it/s]"]},{"output_type":"stream","name":"stdout","text":["Extracting data/MNIST/raw/train-images-idx3-ubyte.gz to data/MNIST/raw\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["\n","Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n","Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to data/MNIST/raw/train-labels-idx1-ubyte.gz\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 28881/28881 [00:00<00:00, 105795365.79it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Extracting data/MNIST/raw/train-labels-idx1-ubyte.gz to data/MNIST/raw\n","\n","Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n","Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1648877/1648877 [00:00<00:00, 149422940.90it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Extracting data/MNIST/raw/t10k-images-idx3-ubyte.gz to data/MNIST/raw\n","\n","Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n","Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 4542/4542 [00:00<00:00, 19399723.80it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Extracting data/MNIST/raw/t10k-labels-idx1-ubyte.gz to data/MNIST/raw\n","\n","Epoch 1\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 10/10 [00:00<00:00, 43.73it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 2\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 10/10 [00:00<00:00, 60.72it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 3\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 10/10 [00:00<00:00, 66.89it/s]\n"]},{"output_type":"error","ename":"AssertionError","evalue":"Sanity check failed: Loss did not decrease after training.","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-6-a58a387dc68e>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest_model_sanity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/content/s6-backprop/Part 2/model.py\u001b[0m in \u001b[0;36mtest_model_sanity\u001b[0;34m()\u001b[0m\n\u001b[1;32m    126\u001b[0m         \u001b[0mfinal_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"sum\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m     \u001b[0;32massert\u001b[0m \u001b[0mfinal_loss\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0minitial_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Sanity check failed: Loss did not decrease after training.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Sanity check passed: Model is capable of overfitting to a small subset of the data.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAssertionError\u001b[0m: Sanity check failed: Loss did not decrease after training."]}]},{"cell_type":"code","source":["# This block of code is used to define transformations for the training dataset.\n","train_transforms = transforms.Compose([\n","    # Randomly applies a center crop of size 22 to the input image with a probability of 0.1.\n","    transforms.RandomApply([transforms.CenterCrop(22), ], p=0.1),\n","    # Resizes the input image to a size of 28x28 pixels.\n","    transforms.Resize((28, 28)),\n","    # Randomly rotates the input image within a range of -15 to 15 degrees. The areas left after rotation are filled with 0 (black).\n","    transforms.RandomRotation((-15., 15.), fill=0),\n","    # Converts the input image to a PyTorch tensor.\n","    transforms.ToTensor(),\n","    # Normalizes the input tensor with a mean of 0.1307 and a standard deviation of 0.3081.\n","    transforms.Normalize((0.1307,), (0.3081,)),\n","])\n","\n","# This block of code is used to define transformations for the testing dataset.\n","test_transforms = transforms.Compose([\n","    # Converts the input image to a PyTorch tensor.\n","    transforms.ToTensor(),\n","    # Normalizes the input tensor with a mean of 0.1307 and a standard deviation of 0.3081.\n","    transforms.Normalize((0.1307,), (0.3081,))\n","])"],"metadata":{"id":"0CVLR-PRI9Qt","executionInfo":{"status":"aborted","timestamp":1709355308210,"user_tz":-240,"elapsed":4,"user":{"displayName":"Aakash Vardhan","userId":"03146811293329168388"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# MNIST Training dataset with specified transformation\n","train_data = datasets.MNIST('../data', train=True, download=True, transform=train_transforms)\n","# MNIST Testing dataset with specified transformation\n","test_data = datasets.MNIST('../data', train=False, download=True, transform=test_transforms)"],"metadata":{"id":"8fP9W6DpAqdm","executionInfo":{"status":"aborted","timestamp":1709355308210,"user_tz":-240,"elapsed":4,"user":{"displayName":"Aakash Vardhan","userId":"03146811293329168388"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Set the batch size to 512, indicating the number of samples to be processed in one go.\n","batch_size = 512\n","\n","# Define a dictionary of keyword arguments for the DataLoader:\n","# 'batch_size': Specifies the number of samples in each batch.\n","# 'shuffle': If True, the dataset will be shuffled at the beginning of each epoch to reduce model overfitting.\n","# 'num_workers': Sets the number of subprocesses to use for data loading. Utilizing multiple workers can enhance data loading throughput.\n","# 'pin_memory': When set to True and using a CUDA-enabled GPU, this option pins memory, potentially speeding up data transfer to the GPU.\n","kwargs = {'batch_size': batch_size, 'shuffle': True, 'num_workers': 2, 'pin_memory': True}\n","\n","# Initialize the DataLoader for the test dataset.\n","# 'test_data': The dataset to use for testing.\n","# The DataLoader handles efficient loading of data from 'test_data' using the parameters defined in 'kwargs'.\n","test_loader = torch.utils.data.DataLoader(test_data, **kwargs)\n","\n","# Initialize the DataLoader for the training dataset.\n","# 'train_data': The dataset to use for training. The DataLoader will shuffle this data if 'shuffle' is True, as per 'kwargs'.\n","# This DataLoader facilitates efficient loading of training data, respecting the parameters specified in 'kwargs'.\n","train_loader = torch.utils.data.DataLoader(train_data, **kwargs)"],"metadata":{"id":"X3b3UJgXAuV3","executionInfo":{"status":"aborted","timestamp":1709355308210,"user_tz":-240,"elapsed":4,"user":{"displayName":"Aakash Vardhan","userId":"03146811293329168388"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Retrieve the first batch of data and labels from the training DataLoader\n","batch_data, batch_label = next(iter(train_loader))\n","\n","# Initialize a new figure for plotting\n","fig = plt.figure()\n","\n","# Loop over the first 12 images and labels in the batch.\n","for i in range(12):\n","  # Create a 3x4 grid of subplots. The index of the current subplot (i+1) is set to active\n","  plt.subplot(3,4,i+1)\n","  # Adjust the layout to prevent overlapping of subplot elements.\n","  plt.tight_layout()\n","  # Display an image. 'squeeze(0)' removes a dimension of size 1, which is common for grayscale images in PyTorch.\n","  plt.imshow(batch_data[i].squeeze(0), cmap='gray')\n","  # Set the title of the subplot to the label of the current image. '.item()' converts a PyTorch scalar to a Python number.\n","  plt.title(batch_label[i].item())\n","  # Remove the x-axis ticks.\n","  plt.xticks([])\n","  # Remove the y-axis ticks.\n","  plt.yticks([])"],"metadata":{"id":"kZmHwqVTAxro","executionInfo":{"status":"aborted","timestamp":1709355308210,"user_tz":-240,"elapsed":4,"user":{"displayName":"Aakash Vardhan","userId":"03146811293329168388"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Initialize the model and move it to the specified device (GPU or CPU)\n","model = Net().to(device)\n","\n","# Set up the optimizer for training. Here, we're using Stochastic Gradient Descent (SGD)\n","# with a learning rate of 0.01 and momentum of 0.9 for better convergence\n","optimizer = optim.SGD(model.parameters(), lr=0.05, momentum=0.9)\n","\n","# Define a learning rate scheduler that decreases the learning rate by a factor of 0.1\n","# every 10 epochs to fine-tune the training in later stages\n","scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1, verbose=True)\n","\n","criterion = F.nll_loss\n","\n","# Specify the number of epochs to train the model\n","num_epochs = 19\n","\n","# Start the training loop over the specified number of epochs\n","for epoch in range(1, num_epochs+1):\n","  # Print the current epoch number\n","  print(f'Epoch {epoch}')\n","\n","  # Call the training function for a single epoch with the training data\n","  # Pass the model, device, training data loader, optimizer, and loss function as arguments\n","  train(model, device, train_loader, optimizer, criterion)\n","\n","  # Evaluate the model with the testing data after training\n","  # Pass the model, device, testing data loader, and loss function as arguments\n","  test(model, device, test_loader, criterion)\n","\n","  # Update the learning rate based on the scheduler\n","  scheduler.step()"],"metadata":{"id":"pU2AsIm9Azuz","executionInfo":{"status":"aborted","timestamp":1709355308211,"user_tz":-240,"elapsed":5,"user":{"displayName":"Aakash Vardhan","userId":"03146811293329168388"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt_fig()"],"metadata":{"id":"2imf0fp2BFK_","executionInfo":{"status":"aborted","timestamp":1709355308211,"user_tz":-240,"elapsed":5,"user":{"displayName":"Aakash Vardhan","userId":"03146811293329168388"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"cF0Vz47OTGWK","executionInfo":{"status":"aborted","timestamp":1709355308211,"user_tz":-240,"elapsed":5,"user":{"displayName":"Aakash Vardhan","userId":"03146811293329168388"}}},"execution_count":null,"outputs":[]}],"metadata":{"language_info":{"name":"python"},"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}